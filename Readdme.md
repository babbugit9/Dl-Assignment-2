## ğŸ§ Deep Learning Assignment 2

*Course:* Deep Learning - 22CAC04\
*Institution:* Chaitanya Bharathi Institute of Technology\
*Department:* Information Technology\
*Due Date:* 20-04-25

### ğŸ” Overview

This repository contains the implementation of *Question 1* and *Question 2* of the Deep Learning Assignment 2.

---

## ğŸ“Œ Question 1: Latin-to-Devanagari Transliteration

### ğŸš€ Objective

To build a flexible RNN-based seq2seq architecture for transliterating Latin script inputs to their corresponding Devanagari script representations. The model supports multiple cell types: *SimpleRNN, **GRU, and **LSTM*, with tunable hyperparameters.

---

### ğŸ—‚ Dataset

Dataset used: [Dakshina Dataset (Google)](https://github.com/google-research-datasets/dakshina)\
Files used:

- hi.translit.sampled.train.tsv
- hi.translit.sampled.dev.tsv
- hi.translit.sampled.test.tsv

Each file contains columns:

- Devanagari script
- Latin transliteration
- Frequency count

---

### ğŸ§± Model Architecture

1. *Embedding Layer* for both encoder and decoder
2. *Encoder RNN (LSTM / GRU / SimpleRNN)* - processes the Latin script input
3. *Decoder RNN (LSTM / GRU / SimpleRNN)* - generates the Devanagari script character-by-character using the final encoder state
4. *Dense Layer* with softmax activation for character prediction

*Flexibility:*

- Embedding Dimension
- Hidden Units
- RNN Cell Type ('lstm', 'gru', 'rnn')
- Number of Layers (extendable in the function)

---

### ğŸ§® Theoretical Analysis

#### a) Total Number of Computations

Let:

- m = embedding dimension
- k = hidden size
- T = sequence length
- V = vocabulary size

Total computations (approx):\
Encoder: O(T Ã— (mÃ—k + kÂ²))\
Decoder: O(T Ã— (mÃ—k + kÂ² + kÃ—V))

#### b) Total Number of Parameters

Encoder LSTM: 4 Ã— (kÃ—(k + m + 1))\
Decoder LSTM: 4 Ã— (kÃ—(k + m + 1))\
Dense Output: k Ã— V\
Embedding Layers: V Ã— m (each for encoder and decoder)

---

### ğŸ“Š Training Details

- *Optimizer:* Adam
- *Loss:* Categorical Crossentropy
- *Batch Size:* 64
- *Epochs:* 30
- *Validation Accuracy:* \~94.6%
- *Test Accuracy:* *0.9457*

---

### ğŸ“ˆ Sample Predictions

| Input (Latin) | Target (Devanagari) | Predicted |
| ------------- | ------------------- | --------- |
| a n k         | à¤… à¤‚ à¤•               | à¤à¤‚à¤•       |
| a n k a       | à¤… à¤‚ à¤•               | à¤…à¤‚à¤•à¤¾      |
| a n k i t     | à¤… à¤‚ à¤• à¤¿ à¤¤           | à¤…à¤‚à¤•à¤¿à¤¤     |
| a n a k o n   | à¤… à¤‚ à¤• à¥‹ à¤‚           | à¤…à¤¨à¤•à¥‹à¤‚     |
| a n k h o n   | à¤… à¤‚ à¤• à¥‹ à¤‚           | à¤…à¤‚à¤–à¥‹à¤‚     |

---

### ğŸ§² Evaluation

bash
Test Accuracy: 0.9457


---

### ğŸ›  How to Run

#### ğŸ”§ Install Requirements

bash
pip install tensorflow==2.12.0 pandas gdown


#### â–¶ Run Training

Ensure the .tsv files from Dakshina dataset are in your working directory.

python
python main_seq2seq_transliteration.py


---

### ğŸ“‚ File Structure


.
â”œâ”€â”€ README.md
â”œâ”€â”€ main_seq2seq_transliteration.py  # All code for Q1
â”œâ”€â”€ hi.translit.sampled.train.tsv
â”œâ”€â”€ hi.translit.sampled.dev.tsv
â””â”€â”€ hi.translit.sampled.test.tsv


---

ğŸ“˜ References

- [Keras LSTM Seq2Seq Example](https://keras.io/examples/nlp/lstm_seq2seq/)
- [Machine Learning Mastery - Seq2Seq](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)
- [Dakshina Dataset](https://github.com/google-research-datasets/dakshina)

---

# ğŸ¤ Fine-Tuned GPT-2 on Poetry Dataset

This project fine-tunes the GPT-2 language model on a poetry dataset to generate creative and lyrical text. It uses TensorFlow and Hugging Face Transformers to train the model and generate diverse versions of lyrics based on a given prompt.

## ğŸ“‚ Dataset

The dataset used is the [Poetry Dataset by Paul Mooney](https://www.kaggle.com/datasets/paultimothymooney/poetry), accessed via KaggleHub. This dataset contains collections of poems from various artists.

Only the `bieber.txt` file was used for this fine-tuning demo.

## ğŸ§  Model

We used the `gpt2` model from Hugging Face's `transformers` library and fine-tuned it using TensorFlow on the selected poetry dataset.

**Model Details:**

- Base model: GPT-2 (`TFGPT2LMHeadModel`)
- Tokenizer: GPT-2 tokenizer with EOS token used as padding
- Input length: 128 tokens
- Training epochs: 3
- Batch size: 4

## ğŸš€ Setup Instructions

1. Clone this repository and open the notebook in Google Colab.

2. Install required libraries:

   ```bash
   pip install kagglehub datasets transformers
   ```

3. Authenticate with Kaggle to access the dataset.

4. Run the notebook cells to:

   - Download the dataset
   - Preprocess and tokenize the text
   - Fine-tune the model
   - Save and generate from the model

## ğŸ‹ï¸â€â™€ï¸ Training

The model was trained over 3 epochs:

```
Epoch 1/3 - loss: 0.2775  
Epoch 2/3 - loss: 0.1929  
Epoch 3/3 - loss: 0.1515  
```

The model converges quickly due to the relatively small dataset.

## ğŸ¶ Poetry Generation

After training, the model can generate creative lyrical continuations based on a prompt. For example, given:

```text
In the moonlight, I feel
My heart begins to heal
The stars are shining bright
I'm floating in the night
```

It generates multiple versions like:

```
ğŸ¶ Version 1:
In the moonlight, I feel
My heart begins to heal
The stars are shining bright
I'm floating in the night
It's all about the magic

ğŸ¶ Version 5:
In the moonlight, I feel
My heart begins to heal
The stars are shining bright
I'm floating in the night
So take my hand
```

## ğŸ“ Output

The fine-tuned model and tokenizer are saved in the directory:

```
./fine_tuned_gpt2_poetry/
```

You can reload the model for inference or further training.

## ğŸ› ï¸ Future Work

- Train on larger or more diverse poetry datasets
- Improve output structure using rhyme detection or meter control
- Convert this into an interactive web app or chatbot

---

## ğŸ‘¤ Author

**Namani Pravardhan**

