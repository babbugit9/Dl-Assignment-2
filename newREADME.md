
# ğŸ§  Deep Learning Assignment - CBIT IT Dept

**Course**: DL (22CAC04)  
**Institution**: CBIT, Hyderabad  
**Branch**: Information Technology  
**Assignment Due**: April 20, 2025

---

## ğŸ“˜ Contents

This repository showcases two key components of Assignment 2:

- ğŸ”¤ **Q1**: Transliteration using Sequence-to-Sequence models  
- âœï¸ **Q2**: Text generation by fine-tuning GPT-2 on poetry

---

## ğŸ”¡ Question 1 - Transliteration from Latin to Devanagari

### ğŸ¯ Goal

Design and implement a configurable sequence-to-sequence model that transliterates Latin script inputs into their corresponding Devanagari outputs. The architecture should be adaptable to various RNN cells and configurations.

### ğŸ“ Dataset - Dakshina by Google

Source: [Google Research - Dakshina](https://github.com/google-research-datasets/dakshina)  
Files used:

- `hi.translit.sampled.train.tsv`
- `hi.translit.sampled.dev.tsv`
- `hi.translit.sampled.test.tsv`

Each line contains:
- Hindi word in Devanagari
- Corresponding Latin script
- Frequency count

---

### ğŸ—ï¸ Model Blueprint

- ğŸ”  Embedding layer (encoder & decoder)
- ğŸ§  Recurrent Cell (RNN / LSTM / GRU)
- ğŸ”„ Encoder processes Latin characters
- ğŸ—£ï¸ Decoder generates Devanagari characters
- ğŸ”š Dense layer with softmax for output prediction

> ğŸ› ï¸ The design supports dynamic changes for:
> - RNN type
> - Embedding dimensions
> - Hidden layer size
> - Vocabulary size

---

### ğŸ”¬ Computation & Parameters

#### (a) Total Computation

For:
- `m =` embedding size
- `k =` hidden dimension
- `T =` sequence length
- `V =` vocabulary size

Computation per time step:  
- Encoder â‰ˆ T Ã— (mÃ—k + kÂ²)  
- Decoder â‰ˆ T Ã— (mÃ—k + kÂ² + kÃ—V)

#### (b) Total Parameters

- Encoder LSTM: 4 Ã— (k Ã— (k + m + 1))  
- Decoder LSTM: 4 Ã— (k Ã— (k + m + 1))  
- Output layer: k Ã— V  
- Embeddings: V Ã— m (encoder + decoder)

---

### ğŸ§ª Performance

- **Optimizer**: Adam  
- **Loss**: Categorical Crossentropy  
- **Epochs**: 30  
- **Batch Size**: 64  
- **Test Accuracy**: `94.57%`

---

### âœ¨ Sample Output

| Latin Input | Expected Output | Model Prediction |
|-------------|-----------------|------------------|
| a n k       | à¤…à¤‚à¤•              | à¤à¤‚à¤•              |
| a n k i t   | à¤…à¤‚à¤•à¤¿à¤¤            | à¤…à¤‚à¤•à¤¿à¤¤             |
| a n k a     | à¤…à¤‚à¤•à¤¾             | à¤…à¤‚à¤•à¤¾              |
| a n a k o n | à¤…à¤¨à¤•à¥‹à¤‚            | à¤…à¤¨à¤•à¥‹à¤‚             |
| a n k h o n | à¤…à¤‚à¤–à¥‹à¤‚            | à¤…à¤‚à¤–à¥‹à¤‚             |

---

### â–¶ï¸ Run Instructions

Install required libraries:

```bash
pip install tensorflow==2.12.0 pandas gdown
```

Run training script (with dataset in root):

```bash
python main_seq2seq_transliteration.py
```

---

## âœ¨ Question 2 - GPT-2 Fine-Tuning for Poetry Generation

### ğŸ“ Summary

This task focuses on creatively generating poetic text by fine-tuning OpenAI's GPT-2 model on a curated poetry dataset using the Transformers library and TensorFlow backend.

### ğŸ“š Dataset

Used: [`poetry` dataset by Paul Mooney (Kaggle)](https://www.kaggle.com/datasets/paultimothymooney/poetry)

- Only the `bieber.txt` file was selected for fine-tuning.

---

### ğŸ§± GPT-2 Configuration

- Model: `gpt2` via Hugging Face
- Tokenizer: GPT-2 with padding token
- Sequence length: 128 tokens
- Epochs: 3
- Batch Size: 4

---

### ğŸ”„ Training Logs

```
Epoch 1 - Loss: 0.2775
Epoch 2 - Loss: 0.1929
Epoch 3 - Loss: 0.1515
```

---

### ğŸ¤ Sample Generation

Prompt:

```text
In the moonlight, I feel
My heart begins to heal
The stars are shining bright
I'm floating in the night
```

Generated:

```
ğŸŒŸ Version 1:
In the moonlight, I feel
My heart begins to heal
The stars are shining bright
I'm floating in the night
Whispers in the dark

ğŸŒŸ Version 2:
In the moonlight, I feel
My heart begins to heal
The stars are shining bright
I'm floating in the night
Calling out your name
```

---

### ğŸ“¦ Output Directory

Trained artifacts are saved in:

```
./fine_tuned_gpt2_poetry/
```

---

## ğŸ”§ Setup Guide

Install requirements:

```bash
pip install kagglehub datasets transformers
```

Follow notebook to:
- Load dataset
- Preprocess
- Fine-tune GPT-2
- Generate poetic output

---

## ğŸ‘¨â€ğŸ’» Author

**Namani Pravardhan**  
CBIT | Dept. of IT  


---
