
## ğŸ§ Deep Learning Assignment 2

*Course:* Deep Learning - 22CAC04  
*Institution:* Chaitanya Bharathi Institute of Technology  
*Department:* Information Technology  
*Due Date:* 20-04-25

### ğŸ” Overview

This repository contains the implementation of *Question 1* and *Question 2* of the Deep Learning Assignment 2.

---

## ğŸ“Œ Question 1: Latin-to-Devanagari Transliteration

### ğŸš€ Objective

To build a flexible RNN-based seq2seq architecture for transliterating Latin script inputs to their corresponding Devanagari script representations. The model supports multiple cell types: *SimpleRNN*, *GRU*, and *LSTM*, with tunable hyperparameters.

---

### ğŸ—‚ Dataset

Dataset used: [Dakshina Dataset (Google)](https://github.com/google-research-datasets/dakshina)  
Files used:

- `hi.translit.sampled.train.tsv`
- `hi.translit.sampled.dev.tsv`
- `hi.translit.sampled.test.tsv`

Each file contains the following columns:

- Devanagari script
- Latin transliteration
- Frequency count

---

### ğŸ§± Model Architecture

1. Embedding layers for both encoder and decoder
2. Encoder RNN (LSTM / GRU / SimpleRNN) to process Latin inputs
3. Decoder RNN (LSTM / GRU / SimpleRNN) to generate Devanagari characters
4. Dense layer with softmax for prediction

**Tunable Parameters:**

- Embedding dimension
- Hidden size
- RNN cell type (`rnn`, `gru`, `lstm`)
- Number of RNN layers

---

### ğŸ§® Theoretical Analysis

#### a) Total Number of Computations

Let:  
- *m* = embedding dimension  
- *k* = hidden size  
- *T* = sequence length  
- *V* = vocabulary size  

**Encoder Computation:** O(T Ã— (mÃ—k + kÂ²))  
**Decoder Computation:** O(T Ã— (mÃ—k + kÂ² + kÃ—V))

#### b) Total Number of Parameters

- Encoder LSTM: 4 Ã— (k Ã— (k + m + 1))  
- Decoder LSTM: 4 Ã— (k Ã— (k + m + 1))  
- Output Dense: k Ã— V  
- Embedding Layers: V Ã— m (for both encoder and decoder)

---

### ğŸ“Š Training Details

- **Optimizer:** Adam  
- **Loss Function:** Categorical Crossentropy  
- **Batch Size:** 64  
- **Epochs:** 30  
- **Validation Accuracy:** ~94.6%  
- **Test Accuracy:** *0.9457*

---

### ğŸ“ˆ Sample Predictions

Below are examples demonstrating the modelâ€™s transliteration capability from Latin input to Devanagari output:

- **Input:** `a n k`  
  **Target:** à¤…à¤‚ à¤•  
  **Predicted:** à¤à¤‚à¤•

- **Input:** `a n k a`  
  **Target:** à¤…à¤‚ à¤• à¤…  
  **Predicted:** à¤…à¤‚à¤•à¤¾

- **Input:** `a n k i t`  
  **Target:** à¤…à¤‚ à¤•à¤¿ à¤¤  
  **Predicted:** à¤…à¤‚à¤•à¤¿à¤¤

- **Input:** `a n a k o n`  
  **Target:** à¤… à¤¨ à¤• à¥‹ à¤¨  
  **Predicted:** à¤…à¤¨à¤•à¥‹à¤‚

- **Input:** `a n k h o n`  
  **Target:** à¤…à¤‚ à¤– à¥‹ à¤‚  
  **Predicted:** à¤…à¤‚à¤–à¥‹à¤‚

---

### ğŸ§² Evaluation

```bash
Test Accuracy: 0.9457
```

---

### ğŸ›  How to Run

#### ğŸ”§ Install Requirements

```bash
pip install tensorflow==2.12.0 pandas gdown
```

#### â–¶ Run Training

Ensure the .tsv files from the Dakshina dataset are placed in your working directory.

```bash
python main_seq2seq_transliteration.py
```

---

### ğŸ“‚ File Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ main_seq2seq_transliteration.py
â”œâ”€â”€ hi.translit.sampled.train.tsv
â”œâ”€â”€ hi.translit.sampled.dev.tsv
â””â”€â”€ hi.translit.sampled.test.tsv
```

---

ğŸ“˜ **References**

- [Keras LSTM Seq2Seq Example](https://keras.io/examples/nlp/lstm_seq2seq/)
- [Machine Learning Mastery - Seq2Seq](https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/)
- [Dakshina Dataset](https://github.com/google-research-datasets/dakshina)

---

# ğŸ¤ Fine-Tuned GPT-2 on Poetry Dataset

This project fine-tunes the GPT-2 language model on a poetry dataset to generate creative and lyrical text. It uses TensorFlow and Hugging Face Transformers to train the model and generate diverse lyrics from a given prompt.

## ğŸ“‚ Dataset

Dataset used: [Poetry Dataset by Paul Mooney](https://www.kaggle.com/datasets/paultimothymooney/poetry)  
For this project, only the `bieber.txt` file was utilized.

## ğŸ§  Model

The model is based on Hugging Face's `gpt2` and was trained using TensorFlow.

**Configuration:**

- Base: GPT-2 (`TFGPT2LMHeadModel`)
- Tokenizer: GPT-2 tokenizer with EOS token padding
- Max input length: 128 tokens
- Training epochs: 3
- Batch size: 4

## ğŸš€ Setup Instructions

1. Clone this repo and open the notebook in Colab.
2. Install the necessary libraries:

```bash
pip install kagglehub datasets transformers
```

3. Authenticate with Kaggle to access the dataset.
4. Run the notebook to preprocess, train, and generate outputs.

## ğŸ‹ï¸â€â™€ï¸ Training Log

```
Epoch 1/3 - loss: 0.2775  
Epoch 2/3 - loss: 0.1929  
Epoch 3/3 - loss: 0.1515  
```

## ğŸ¶ Sample Generations

Given this prompt:

```text
In the moonlight, I feel  
My heart begins to heal  
The stars are shining bright  
I'm floating in the night  
```

The model outputs:

- ğŸ¶ *Version 1:*  
  In the moonlight, I feel  
  My heart begins to heal  
  The stars are shining bright  
  I'm floating in the night  
  It's all about the magic  

- ğŸ¶ *Version 2:*  
  In the moonlight, I feel  
  My heart begins to heal  
  The stars are shining bright  
  I'm floating in the night  
  So take my hand

## ğŸ“ Output

Model saved at:

```
./fine_tuned_gpt2_poetry/
```

## ğŸ”® Future Enhancements

- Train on a larger or more stylistically diverse poetry corpus
- Add rhyme/meter-based constraints for improved poetic structure
- Deploy as an interactive app or web-based lyric assistant

---

## ğŸ‘¤ Author

**Namani Pravardhan**
