{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBefaXut9OmYAmhlJ1+JJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babbugit9/Dl-Assignment-2/blob/main/Assignment%202%20again.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0\n",
        "!pip install pandas\n",
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCgZ4RqTGldu",
        "outputId": "5562a0ec-4024-4945-cf18-26892790e896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, GRU, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "OD5UgDTZHA03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fetch_translit_data():\n",
        "    file_paths = {\n",
        "        \"training\": \"/content/hi.translit.sampled.train.tsv\",\n",
        "        \"validation\": \"/content/hi.translit.sampled.dev.tsv\",\n",
        "        \"testing\": \"/content/hi.translit.sampled.test.tsv\"\n",
        "    }\n",
        "\n",
        "    def read_file(path):\n",
        "        return pd.read_csv(path, delimiter='\\t', header=None, names=[\"target_script\", \"source_script\", \"frequency\"])\n",
        "\n",
        "    dataset_train = read_file(file_paths[\"training\"])\n",
        "    dataset_dev = read_file(file_paths[\"validation\"])\n",
        "    dataset_test = read_file(file_paths[\"testing\"])\n",
        "\n",
        "    return dataset_train, dataset_dev, dataset_test\n",
        "\n",
        "train_df, dev_df, test_df = fetch_translit_data()\n",
        "\n",
        "print(\"Few samples from training dataset:\")\n",
        "print(train_df.sample(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpTngdXtHKKW",
        "outputId": "e6f9d05f-805f-42c2-f300-ba4e603d77fa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few samples from training dataset:\n",
            "      target_script   source_script  frequency\n",
            "4527        उज्ज्वल          ujjwal          1\n",
            "3451     आवश्यकताएं  aavshyaktaayen          1\n",
            "633        अद्वितीय        advitiya          2\n",
            "43208       हटिंगटन      hatingatan          1\n",
            "33701   राष्ट्रवादी     rashtrawadi          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def prepare_texts(train, dev, test, seq_len=20):\n",
        "    # Drop any missing entries\n",
        "    train = train.dropna()\n",
        "    dev = dev.dropna()\n",
        "    test = test.dropna()\n",
        "\n",
        "    # Rename for consistency\n",
        "    train = train.rename(columns={\"source_script\": \"latin\", \"target_script\": \"hindi\"})\n",
        "    dev = dev.rename(columns={\"source_script\": \"latin\", \"target_script\": \"hindi\"})\n",
        "    test = test.rename(columns={\"source_script\": \"latin\", \"target_script\": \"hindi\"})\n",
        "\n",
        "    # Ensure text columns are string\n",
        "    for df in [train, dev, test]:\n",
        "        df[\"latin\"] = df[\"latin\"].astype(str)\n",
        "        df[\"hindi\"] = df[\"hindi\"].astype(str)\n",
        "\n",
        "    # Combine for tokenizer training\n",
        "    all_latin = pd.concat([train[\"latin\"], dev[\"latin\"], test[\"latin\"]])\n",
        "    all_hindi = pd.concat([train[\"hindi\"], dev[\"hindi\"], test[\"hindi\"]])\n",
        "\n",
        "    latin_tokenizer = Tokenizer(char_level=True)\n",
        "    hindi_tokenizer = Tokenizer(char_level=True)\n",
        "    latin_tokenizer.fit_on_texts(all_latin)\n",
        "    hindi_tokenizer.fit_on_texts(all_hindi)\n",
        "\n",
        "    def encode_sequences(texts, tokenizer, maxlen):\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        return pad_sequences(seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "    def encode_targets(texts, tokenizer, maxlen):\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        start_token = tokenizer.word_index.get('', 0)\n",
        "        seq = [[start_token] + s + [start_token] for s in seq]\n",
        "        return pad_sequences(seq, maxlen=maxlen + 2, padding='post')\n",
        "\n",
        "    # Input sequences\n",
        "    X_train = encode_sequences(train[\"latin\"], latin_tokenizer, seq_len)\n",
        "    X_val = encode_sequences(dev[\"latin\"], latin_tokenizer, seq_len)\n",
        "    X_test = encode_sequences(test[\"latin\"], latin_tokenizer, seq_len)\n",
        "\n",
        "    # Target sequences\n",
        "    y_train = encode_targets(train[\"hindi\"], hindi_tokenizer, seq_len)\n",
        "    y_val = encode_targets(dev[\"hindi\"], hindi_tokenizer, seq_len)\n",
        "    y_test = encode_targets(test[\"hindi\"], hindi_tokenizer, seq_len)\n",
        "\n",
        "    # Decoder input/output\n",
        "    decoder_input_train = y_train[:, :-1]\n",
        "    decoder_target_train = y_train[:, 1:]\n",
        "\n",
        "    decoder_input_val = y_val[:, :-1]\n",
        "    decoder_target_val = y_val[:, 1:]\n",
        "\n",
        "    vocab_size = len(hindi_tokenizer.word_index) + 1\n",
        "\n",
        "    def one_hot(sequences, vocab_size):\n",
        "        return np.array([tf.keras.utils.to_categorical(s, num_classes=vocab_size) for s in sequences])\n",
        "\n",
        "    decoder_target_train = one_hot(decoder_target_train, vocab_size)\n",
        "    decoder_target_val = one_hot(decoder_target_val, vocab_size)\n",
        "\n",
        "    return (X_train, decoder_input_train, decoder_target_train,\n",
        "            X_val, decoder_input_val, decoder_target_val,\n",
        "            latin_tokenizer, hindi_tokenizer)\n"
      ],
      "metadata": {
        "id": "LCOJ5ghtSASR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_translit_model(src_vocab, tgt_vocab, embedding_size=50, hidden_units=128, rnn_type='gru'):\n",
        "    # Encoder\n",
        "    inp_encoder = Input(shape=(None,))\n",
        "    embed_enc = Embedding(src_vocab, embedding_size)(inp_encoder)\n",
        "\n",
        "    if rnn_type == 'lstm':\n",
        "        encoder_outputs, state_h, state_c = LSTM(hidden_units, return_state=True)(embed_enc)\n",
        "        enc_states = [state_h, state_c]\n",
        "    elif rnn_type == 'gru':\n",
        "        encoder_outputs, state_h = GRU(hidden_units, return_state=True)(embed_enc)\n",
        "        enc_states = [state_h]\n",
        "    else:\n",
        "        encoder_outputs, state_h = SimpleRNN(hidden_units, return_state=True)(embed_enc)\n",
        "        enc_states = [state_h]\n",
        "\n",
        "    # Decoder\n",
        "    inp_decoder = Input(shape=(None,))\n",
        "    embed_dec = Embedding(tgt_vocab, embedding_size)(inp_decoder)\n",
        "\n",
        "    if rnn_type == 'lstm':\n",
        "        dec_rnn = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        dec_outputs, _, _ = dec_rnn(embed_dec, initial_state=enc_states)\n",
        "    elif rnn_type == 'gru':\n",
        "        dec_rnn = GRU(hidden_units, return_sequences=True, return_state=True)\n",
        "        dec_outputs, _ = dec_rnn(embed_dec, initial_state=enc_states)\n",
        "    else:\n",
        "        dec_rnn = SimpleRNN(hidden_units, return_sequences=True, return_state=True)\n",
        "        dec_outputs, _ = dec_rnn(embed_dec, initial_state=enc_states)\n",
        "\n",
        "    final_dense = Dense(tgt_vocab, activation='softmax')\n",
        "    dec_outputs = final_dense(dec_outputs)\n",
        "\n",
        "    model = Model([inp_encoder, inp_decoder], dec_outputs)\n",
        "\n",
        "    # Inference models\n",
        "    encoder_model = Model(inp_encoder, enc_states)\n",
        "\n",
        "    decoder_state_inputs = [Input(shape=(hidden_units,)) for _ in enc_states]\n",
        "    embedded_inf_dec = Embedding(tgt_vocab, embedding_size)(inp_decoder)\n",
        "\n",
        "    if rnn_type == 'lstm':\n",
        "        dec_inf_out, *dec_inf_states = dec_rnn(embedded_inf_dec, initial_state=decoder_state_inputs)\n",
        "    else:\n",
        "        dec_inf_out, *dec_inf_states = dec_rnn(embedded_inf_dec, initial_state=decoder_state_inputs)\n",
        "\n",
        "    dec_inf_out = final_dense(dec_inf_out)\n",
        "    decoder_model = Model([inp_decoder] + decoder_state_inputs, [dec_inf_out] + dec_inf_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n"
      ],
      "metadata": {
        "id": "y1-quuX5SWs4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack all three models: training model, encoder model, decoder model\n",
        "model, encoder_model, decoder_model = make_translit_model(\n",
        "    len(latin_token.word_index) + 1,\n",
        "    len(hindi_token.word_index) + 1,\n",
        "    embedding_size=64,\n",
        "    hidden_units=128,\n",
        "    rnn_type='lstm'  # can be 'gru' or 'rnn' as well\n",
        ")\n",
        "\n",
        "# Compile the training model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the training model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    [X_train, dec_in_train],\n",
        "    dec_out_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_data=([X_val, dec_in_val], dec_out_val)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AADSNUA6Sa0U",
        "outputId": "f67399b7-56fc-4c3d-d44e-86768fa5aa25"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, None, 64)     1728        ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, None, 64)     4096        ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, 128),        98816       ['embedding_8[0][0]']            \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, None, 128),  98816       ['embedding_9[0][0]',            \n",
            "                                 (None, 128),                     'lstm_6[0][1]',                 \n",
            "                                 (None, 128)]                     'lstm_6[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, None, 64)     8256        ['lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 211,712\n",
            "Trainable params: 211,712\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "691/691 [==============================] - 86s 118ms/step - loss: 1.1081 - accuracy: 0.7260 - val_loss: 0.9326 - val_accuracy: 0.7493\n",
            "Epoch 2/30\n",
            "691/691 [==============================] - 83s 121ms/step - loss: 0.8909 - accuracy: 0.7566 - val_loss: 0.8226 - val_accuracy: 0.7719\n",
            "Epoch 3/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.7462 - accuracy: 0.7888 - val_loss: 0.6065 - val_accuracy: 0.8224\n",
            "Epoch 4/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.5270 - accuracy: 0.8448 - val_loss: 0.4365 - val_accuracy: 0.8688\n",
            "Epoch 5/30\n",
            "691/691 [==============================] - 82s 119ms/step - loss: 0.3912 - accuracy: 0.8810 - val_loss: 0.3368 - val_accuracy: 0.8973\n",
            "Epoch 6/30\n",
            "691/691 [==============================] - 85s 123ms/step - loss: 0.3106 - accuracy: 0.9039 - val_loss: 0.2820 - val_accuracy: 0.9138\n",
            "Epoch 7/30\n",
            "691/691 [==============================] - 81s 118ms/step - loss: 0.2624 - accuracy: 0.9182 - val_loss: 0.2517 - val_accuracy: 0.9216\n",
            "Epoch 8/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.2308 - accuracy: 0.9280 - val_loss: 0.2312 - val_accuracy: 0.9289\n",
            "Epoch 9/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.2074 - accuracy: 0.9351 - val_loss: 0.2159 - val_accuracy: 0.9323\n",
            "Epoch 10/30\n",
            "691/691 [==============================] - 84s 121ms/step - loss: 0.1899 - accuracy: 0.9405 - val_loss: 0.2062 - val_accuracy: 0.9353\n",
            "Epoch 11/30\n",
            "691/691 [==============================] - 81s 118ms/step - loss: 0.1754 - accuracy: 0.9449 - val_loss: 0.1967 - val_accuracy: 0.9375\n",
            "Epoch 12/30\n",
            "691/691 [==============================] - 82s 119ms/step - loss: 0.1637 - accuracy: 0.9484 - val_loss: 0.1903 - val_accuracy: 0.9401\n",
            "Epoch 13/30\n",
            "691/691 [==============================] - 82s 118ms/step - loss: 0.1533 - accuracy: 0.9516 - val_loss: 0.1901 - val_accuracy: 0.9399\n",
            "Epoch 14/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.1441 - accuracy: 0.9545 - val_loss: 0.1847 - val_accuracy: 0.9419\n",
            "Epoch 15/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.1364 - accuracy: 0.9571 - val_loss: 0.1827 - val_accuracy: 0.9429\n",
            "Epoch 16/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.1295 - accuracy: 0.9593 - val_loss: 0.1835 - val_accuracy: 0.9429\n",
            "Epoch 17/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.1228 - accuracy: 0.9614 - val_loss: 0.1839 - val_accuracy: 0.9421\n",
            "Epoch 18/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.1170 - accuracy: 0.9631 - val_loss: 0.1787 - val_accuracy: 0.9447\n",
            "Epoch 19/30\n",
            "691/691 [==============================] - 81s 118ms/step - loss: 0.1120 - accuracy: 0.9647 - val_loss: 0.1772 - val_accuracy: 0.9461\n",
            "Epoch 20/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.1066 - accuracy: 0.9665 - val_loss: 0.1784 - val_accuracy: 0.9458\n",
            "Epoch 21/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.1020 - accuracy: 0.9681 - val_loss: 0.1806 - val_accuracy: 0.9446\n",
            "Epoch 22/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.0979 - accuracy: 0.9692 - val_loss: 0.1838 - val_accuracy: 0.9436\n",
            "Epoch 23/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.0938 - accuracy: 0.9706 - val_loss: 0.1833 - val_accuracy: 0.9446\n",
            "Epoch 24/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.0896 - accuracy: 0.9718 - val_loss: 0.1853 - val_accuracy: 0.9446\n",
            "Epoch 25/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.0866 - accuracy: 0.9728 - val_loss: 0.1854 - val_accuracy: 0.9450\n",
            "Epoch 26/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.0827 - accuracy: 0.9741 - val_loss: 0.1860 - val_accuracy: 0.9454\n",
            "Epoch 27/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.0797 - accuracy: 0.9749 - val_loss: 0.1879 - val_accuracy: 0.9454\n",
            "Epoch 28/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.0768 - accuracy: 0.9759 - val_loss: 0.1903 - val_accuracy: 0.9452\n",
            "Epoch 29/30\n",
            "691/691 [==============================] - 80s 116ms/step - loss: 0.0737 - accuracy: 0.9770 - val_loss: 0.1926 - val_accuracy: 0.9442\n",
            "Epoch 30/30\n",
            "691/691 [==============================] - 81s 117ms/step - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.1957 - val_accuracy: 0.9446\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x799c2dd0cf50>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transliterate_word(input_seq, encoder_model, decoder_model, tokenizer_src, tokenizer_tgt, max_len=20):\n",
        "    states = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    start_token = tokenizer_tgt.word_index.get('', 0)\n",
        "    target_seq[0, 0] = start_token\n",
        "\n",
        "    reverse_index = {v: k for k, v in tokenizer_tgt.word_index.items()}\n",
        "    output_chars = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        decoder_inputs = [target_seq] + states\n",
        "        preds = decoder_model.predict(decoder_inputs)\n",
        "        token_index = np.argmax(preds[0][0, -1, :])\n",
        "        predicted_char = reverse_index.get(token_index, '')\n",
        "\n",
        "        if predicted_char == '':\n",
        "            break\n",
        "\n",
        "        output_chars.append(predicted_char)\n",
        "        target_seq[0, 0] = token_index\n",
        "        states = preds[1:]\n",
        "\n",
        "    return ''.join(output_chars)\n",
        "\n"
      ],
      "metadata": {
        "id": "_a1qLXKXSfUI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nExample predictions:\")\n",
        "for idx in range(5):\n",
        "    input_seq = X_val[idx:idx+1]\n",
        "    prediction = transliterate_word(input_seq, encoder_model, decoder_model, latin_token, hindi_token)\n",
        "\n",
        "    source_input = val_df['latin'].iloc[idx]\n",
        "    target_output = val_df['hindi'].iloc[idx]\n",
        "\n",
        "    print(f\"Input (Latin): {source_input}\")\n",
        "    print(f\"Target (Devanagari): {target_output}\")\n",
        "    print(f\"Predicted (Devanagari): {prediction}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-sFfh1DfQae",
        "outputId": "c6825652-574e-4229-b91d-9e008560f929"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example predictions:\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "Input (Latin): ankan\n",
            "Target (Devanagari): अंकन\n",
            "Predicted (Devanagari): अंणण\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Input (Latin): angkor\n",
            "Target (Devanagari): अंगकोर\n",
            "Predicted (Devanagari): अंंको\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Input (Latin): angira\n",
            "Target (Devanagari): अंगिरा\n",
            "Predicted (Devanagari): अंगररा\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Input (Latin): angithi\n",
            "Target (Devanagari): अंगीठी\n",
            "Predicted (Devanagari): अंििथ\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Input (Latin): angrej\n",
            "Target (Devanagari): अंग्रेज\n",
            "Predicted (Devanagari): अंरगज\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}