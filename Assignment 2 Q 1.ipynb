{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rZUdAwjH1wBB",
        "outputId": "f17045e4-ac9c-4625-c463-0aba727f35e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.5 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.6 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f0f026058830456d9778bcedaae52455"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 362, in run\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.12.0\n",
        "!pip install pandas\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "HFa1PSiV2Fts"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Hindi data\n",
        "def load_hindi_data():\n",
        "    train_path = \"/content/hi.translit.sampled.train.tsv\"\n",
        "    dev_path = \"/content/hi.translit.sampled.dev.tsv\"\n",
        "    test_path = \"/content/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "    train_data = pd.read_csv(train_path, sep='\\t', header=None,\n",
        "                           names=['devanagari', 'latin', 'count'])\n",
        "    dev_data = pd.read_csv(dev_path, sep='\\t', header=None,\n",
        "                         names=['devanagari', 'latin', 'count'])\n",
        "    test_data = pd.read_csv(test_path, sep='\\t', header=None,\n",
        "                          names=['devanagari', 'latin', 'count'])\n",
        "\n",
        "    return train_data, dev_data, test_data\n",
        "\n",
        "train_data, dev_data, test_data = load_hindi_data()\n",
        "\n",
        "# Show some samples\n",
        "print(\"Sample training data:\")\n",
        "print(train_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKP3nmaG2PcL",
        "outputId": "802041dd-a5fb-4beb-ae78-b00880367ac2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample training data:\n",
            "  devanagari     latin  count\n",
            "0         अं        an      3\n",
            "1    अंकगणित  ankganit      3\n",
            "2       अंकल     uncle      4\n",
            "3      अंकुर     ankur      4\n",
            "4     अंकुरण   ankuran      3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(train_data, dev_data, test_data, max_sequence_length=20):\n",
        "    # First, clean the data by removing any rows with NaN values\n",
        "    train_data = train_data.dropna()\n",
        "    dev_data = dev_data.dropna()\n",
        "    test_data = test_data.dropna()\n",
        "\n",
        "    # Combine all data for vocabulary creation\n",
        "    all_latin = pd.concat([train_data['latin'], dev_data['latin'], test_data['latin']])\n",
        "    all_devanagari = pd.concat([train_data['devanagari'], dev_data['devanagari'], test_data['devanagari']])\n",
        "\n",
        "    # Convert to string type to ensure we don't have any numeric values\n",
        "    all_latin = all_latin.astype(str)\n",
        "    all_devanagari = all_devanagari.astype(str)\n",
        "\n",
        "    # Create character-level tokenizers\n",
        "    latin_tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "    latin_tokenizer.fit_on_texts(all_latin)\n",
        "\n",
        "    devanagari_tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "    devanagari_tokenizer.fit_on_texts(all_devanagari)\n",
        "\n",
        "    # Add start and end tokens for decoder sequences\n",
        "    devanagari_tokenizer.word_index['<start>'] = len(devanagari_tokenizer.word_index) + 1\n",
        "    devanagari_tokenizer.word_index['<end>'] = len(devanagari_tokenizer.word_index) + 1\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    def process_sequences(texts, tokenizer, max_len):\n",
        "        # Ensure all texts are strings\n",
        "        texts = [str(text) for text in texts]\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        seq = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "        return seq\n",
        "\n",
        "    # Process input (Latin) sequences\n",
        "    X_train = process_sequences(train_data['latin'], latin_tokenizer, max_sequence_length)\n",
        "    X_dev = process_sequences(dev_data['latin'], latin_tokenizer, max_sequence_length)\n",
        "    X_test = process_sequences(test_data['latin'], latin_tokenizer, max_sequence_length)\n",
        "\n",
        "    # Process target (Devanagari) sequences with start/end tokens\n",
        "    def process_target_sequences(texts, tokenizer, max_len):\n",
        "        # Ensure all texts are strings\n",
        "        texts = [str(text) for text in texts]\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        # Add start and end tokens\n",
        "        seq = [[tokenizer.word_index['<start>']] + s + [tokenizer.word_index['<end>']] for s in seq]\n",
        "        seq = pad_sequences(seq, maxlen=max_len+2, padding='post')  # +2 for start/end tokens\n",
        "        return seq\n",
        "\n",
        "    y_train = process_target_sequences(train_data['devanagari'], devanagari_tokenizer, max_sequence_length)\n",
        "    y_dev = process_target_sequences(dev_data['devanagari'], devanagari_tokenizer, max_sequence_length)\n",
        "    y_test = process_target_sequences(test_data['devanagari'], devanagari_tokenizer, max_sequence_length)\n",
        "\n",
        "    # Create decoder input (shifted by one) and output data\n",
        "    decoder_input_train = y_train[:, :-1]\n",
        "    decoder_output_train = y_train[:, 1:]\n",
        "\n",
        "    decoder_input_dev = y_dev[:, :-1]\n",
        "    decoder_output_dev = y_dev[:, 1:]\n",
        "\n",
        "    decoder_input_test = y_test[:, :-1]\n",
        "    decoder_output_test = y_test[:, 1:]\n",
        "\n",
        "    # One-hot encode the output\n",
        "    def one_hot_encode(sequences, vocab_size):\n",
        "        return np.array([tf.keras.utils.to_categorical(s, num_classes=vocab_size) for s in sequences])\n",
        "\n",
        "    vocab_size = len(devanagari_tokenizer.word_index)+1  # +1 for 0 padding\n",
        "\n",
        "    decoder_output_train = one_hot_encode(decoder_output_train, vocab_size)\n",
        "    decoder_output_dev = one_hot_encode(decoder_output_dev, vocab_size)\n",
        "    decoder_output_test = one_hot_encode(decoder_output_test, vocab_size)\n",
        "\n",
        "    return (X_train, decoder_input_train, decoder_output_train,\n",
        "            X_dev, decoder_input_dev, decoder_output_dev,\n",
        "            X_test, decoder_input_test, decoder_output_test,\n",
        "            latin_tokenizer, devanagari_tokenizer)\n",
        "\n",
        "(X_train, decoder_input_train, decoder_output_train,\n",
        " X_dev, decoder_input_dev, decoder_output_dev,\n",
        " X_test, decoder_input_test, decoder_output_test,\n",
        " latin_tokenizer, devanagari_tokenizer) = preprocess_data(train_data, dev_data, test_data)"
      ],
      "metadata": {
        "id": "E_-3UHVO2PYk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_seq2seq_model(input_vocab_size, target_vocab_size, embedding_dim=64,\n",
        "                       hidden_units=128, cell_type='lstm'):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "\n",
        "    # Choose RNN cell\n",
        "    if cell_type == 'lstm':\n",
        "        encoder_rnn = LSTM(hidden_units, return_state=True)\n",
        "        encoder_outputs, state_h, state_c = encoder_rnn(encoder_embedding)\n",
        "        encoder_states = [state_h, state_c]\n",
        "    elif cell_type == 'gru':\n",
        "        encoder_rnn = GRU(hidden_units, return_state=True)\n",
        "        encoder_outputs, state_h = encoder_rnn(encoder_embedding)\n",
        "        encoder_states = [state_h]\n",
        "    else:  # SimpleRNN\n",
        "        encoder_rnn = SimpleRNN(hidden_units, return_state=True)\n",
        "        encoder_outputs, state_h = encoder_rnn(encoder_embedding)\n",
        "        encoder_states = [state_h]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "\n",
        "    if cell_type == 'lstm':\n",
        "        decoder_rnn = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "    elif cell_type == 'gru':\n",
        "        decoder_rnn = GRU(hidden_units, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "    else:\n",
        "        decoder_rnn = SimpleRNN(hidden_units, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _ = decoder_rnn(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Training model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Inference models\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_inputs = [Input(shape=(hidden_units,)) for _ in encoder_states]\n",
        "    decoder_outputs, *decoder_states = decoder_rnn(\n",
        "        decoder_embedding, initial_state=decoder_state_inputs)\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_state_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "# Build the model\n",
        "input_vocab_size = len(latin_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(devanagari_tokenizer.word_index) + 1\n",
        "\n",
        "model, encoder_model, decoder_model = build_seq2seq_model(\n",
        "    input_vocab_size, target_vocab_size,\n",
        "    embedding_dim=64, hidden_units=128, cell_type='lstm')\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q693juL42PVz",
        "outputId": "3077e280-6fd2-40c4-92f6-ed616cf9b377"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 64)     1728        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 64)     4224        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 128),        98816       ['embedding[0][0]']              \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 128),  98816       ['embedding_1[0][0]',            \n",
            "                                 (None, 128),                     'lstm[0][1]',                   \n",
            "                                 (None, 128)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 66)     8514        ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 212,098\n",
            "Trainable params: 212,098\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train, decoder_input_train],\n",
        "    decoder_output_train,\n",
        "    batch_size=64,\n",
        "    epochs=30,\n",
        "    validation_data=([X_dev, decoder_input_dev], decoder_output_dev),\n",
        "    verbose=1\n",
        ")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzrf77o62PGC",
        "outputId": "e1336258-0b99-4e6a-8be6-028cb483266d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "691/691 [==============================] - 63s 86ms/step - loss: 1.1305 - accuracy: 0.7196 - val_loss: 0.9317 - val_accuracy: 0.7501\n",
            "Epoch 2/30\n",
            "691/691 [==============================] - 58s 83ms/step - loss: 0.8879 - accuracy: 0.7565 - val_loss: 0.7977 - val_accuracy: 0.7765\n",
            "Epoch 3/30\n",
            "691/691 [==============================] - 58s 85ms/step - loss: 0.7276 - accuracy: 0.7926 - val_loss: 0.5940 - val_accuracy: 0.8277\n",
            "Epoch 4/30\n",
            "691/691 [==============================] - 61s 89ms/step - loss: 0.5186 - accuracy: 0.8460 - val_loss: 0.4163 - val_accuracy: 0.8742\n",
            "Epoch 5/30\n",
            "691/691 [==============================] - 57s 83ms/step - loss: 0.3783 - accuracy: 0.8845 - val_loss: 0.3304 - val_accuracy: 0.8979\n",
            "Epoch 6/30\n",
            "691/691 [==============================] - 57s 83ms/step - loss: 0.3053 - accuracy: 0.9056 - val_loss: 0.2795 - val_accuracy: 0.9122\n",
            "Epoch 7/30\n",
            "691/691 [==============================] - 56s 81ms/step - loss: 0.2606 - accuracy: 0.9188 - val_loss: 0.2497 - val_accuracy: 0.9221\n",
            "Epoch 8/30\n",
            "691/691 [==============================] - 57s 82ms/step - loss: 0.2304 - accuracy: 0.9280 - val_loss: 0.2371 - val_accuracy: 0.9256\n",
            "Epoch 9/30\n",
            "691/691 [==============================] - 55s 80ms/step - loss: 0.2077 - accuracy: 0.9349 - val_loss: 0.2172 - val_accuracy: 0.9311\n",
            "Epoch 10/30\n",
            "691/691 [==============================] - 58s 83ms/step - loss: 0.1893 - accuracy: 0.9406 - val_loss: 0.2113 - val_accuracy: 0.9336\n",
            "Epoch 11/30\n",
            "691/691 [==============================] - 56s 81ms/step - loss: 0.1745 - accuracy: 0.9451 - val_loss: 0.1984 - val_accuracy: 0.9373\n",
            "Epoch 12/30\n",
            "691/691 [==============================] - 56s 82ms/step - loss: 0.1623 - accuracy: 0.9489 - val_loss: 0.1922 - val_accuracy: 0.9396\n",
            "Epoch 13/30\n",
            "691/691 [==============================] - 56s 80ms/step - loss: 0.1519 - accuracy: 0.9521 - val_loss: 0.1895 - val_accuracy: 0.9397\n",
            "Epoch 14/30\n",
            "691/691 [==============================] - 58s 83ms/step - loss: 0.1427 - accuracy: 0.9548 - val_loss: 0.1861 - val_accuracy: 0.9406\n",
            "Epoch 15/30\n",
            "691/691 [==============================] - 55s 79ms/step - loss: 0.1349 - accuracy: 0.9574 - val_loss: 0.1834 - val_accuracy: 0.9413\n",
            "Epoch 16/30\n",
            "691/691 [==============================] - 56s 81ms/step - loss: 0.1279 - accuracy: 0.9597 - val_loss: 0.1825 - val_accuracy: 0.9423\n",
            "Epoch 17/30\n",
            "691/691 [==============================] - 55s 80ms/step - loss: 0.1218 - accuracy: 0.9613 - val_loss: 0.1790 - val_accuracy: 0.9440\n",
            "Epoch 18/30\n",
            "691/691 [==============================] - 58s 84ms/step - loss: 0.1157 - accuracy: 0.9635 - val_loss: 0.1835 - val_accuracy: 0.9427\n",
            "Epoch 19/30\n",
            "691/691 [==============================] - 58s 84ms/step - loss: 0.1105 - accuracy: 0.9652 - val_loss: 0.1821 - val_accuracy: 0.9438\n",
            "Epoch 20/30\n",
            "691/691 [==============================] - 58s 84ms/step - loss: 0.1054 - accuracy: 0.9668 - val_loss: 0.1797 - val_accuracy: 0.9449\n",
            "Epoch 21/30\n",
            "691/691 [==============================] - 57s 82ms/step - loss: 0.1009 - accuracy: 0.9682 - val_loss: 0.1838 - val_accuracy: 0.9435\n",
            "Epoch 22/30\n",
            "691/691 [==============================] - 58s 84ms/step - loss: 0.0969 - accuracy: 0.9694 - val_loss: 0.1839 - val_accuracy: 0.9440\n",
            "Epoch 23/30\n",
            "691/691 [==============================] - 59s 86ms/step - loss: 0.0925 - accuracy: 0.9708 - val_loss: 0.1861 - val_accuracy: 0.9444\n",
            "Epoch 24/30\n",
            "691/691 [==============================] - 59s 86ms/step - loss: 0.0891 - accuracy: 0.9720 - val_loss: 0.1876 - val_accuracy: 0.9433\n",
            "Epoch 25/30\n",
            "691/691 [==============================] - 59s 85ms/step - loss: 0.0855 - accuracy: 0.9730 - val_loss: 0.1869 - val_accuracy: 0.9441\n",
            "Epoch 26/30\n",
            "691/691 [==============================] - 56s 81ms/step - loss: 0.0820 - accuracy: 0.9741 - val_loss: 0.1916 - val_accuracy: 0.9443\n",
            "Epoch 27/30\n",
            "691/691 [==============================] - 56s 81ms/step - loss: 0.0789 - accuracy: 0.9751 - val_loss: 0.1943 - val_accuracy: 0.9435\n",
            "Epoch 28/30\n",
            "691/691 [==============================] - 55s 80ms/step - loss: 0.0759 - accuracy: 0.9761 - val_loss: 0.1935 - val_accuracy: 0.9434\n",
            "Epoch 29/30\n",
            "691/691 [==============================] - 59s 86ms/step - loss: 0.0731 - accuracy: 0.9772 - val_loss: 0.1962 - val_accuracy: 0.9436\n",
            "Epoch 30/30\n",
            "691/691 [==============================] - 56s 81ms/step - loss: 0.0705 - accuracy: 0.9779 - val_loss: 0.1979 - val_accuracy: 0.9440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model,\n",
        "                   latin_tokenizer, devanagari_tokenizer, max_length=20):\n",
        "    # Encode input\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = devanagari_tokenizer.word_index['<start>']\n",
        "\n",
        "    reverse_target_char_index = {i: char for char, i in devanagari_tokenizer.word_index.items()}\n",
        "\n",
        "    decoded_sentence = []\n",
        "    for _ in range(max_length):\n",
        "        output_tokens, *states_value = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence.append(sampled_char)\n",
        "\n",
        "        if sampled_char == '<end>':\n",
        "            break\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return ''.join([c for c in decoded_sentence if c not in ['<start>', '<end>']])\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate([X_test, decoder_input_test], decoder_output_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Show some predictions\n",
        "for i in range(5):\n",
        "    input_seq = X_test[i:i+1]\n",
        "    decoded = decode_sequence(\n",
        "        input_seq, encoder_model, decoder_model,\n",
        "        latin_tokenizer, devanagari_tokenizer)\n",
        "\n",
        "    original_input = latin_tokenizer.sequences_to_texts([X_test[i]])[0]\n",
        "    original_target = devanagari_tokenizer.sequences_to_texts([decoder_input_test[i]])[0]\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Input (Latin): {original_input}\")\n",
        "    print(f\"Target (Devanagari): {original_target}\")\n",
        "    print(f\"Predicted (Devanagari): {decoded}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-zriEXG2qKp",
        "outputId": "1f6d5949-674c-401b-9254-99dbab9fd840"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9443\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 391ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "\n",
            "Sample 1:\n",
            "Input (Latin): a n k\n",
            "Target (Devanagari): अ ं क\n",
            "Predicted (Devanagari): एंक\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "\n",
            "Sample 2:\n",
            "Input (Latin): a n k a\n",
            "Target (Devanagari): अ ं क\n",
            "Predicted (Devanagari): अंका\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "\n",
            "Sample 3:\n",
            "Input (Latin): a n k i t\n",
            "Target (Devanagari): अ ं क ि त\n",
            "Predicted (Devanagari): अनकित\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "\n",
            "Sample 4:\n",
            "Input (Latin): a n a k o n\n",
            "Target (Devanagari): अ ं क ो ं\n",
            "Predicted (Devanagari): अनाकों\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "\n",
            "Sample 5:\n",
            "Input (Latin): a n k h o n\n",
            "Target (Devanagari): अ ं क ो ं\n",
            "Predicted (Devanagari): अंखों\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hLdeXO22PCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}